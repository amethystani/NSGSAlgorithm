# -*- coding: utf-8 -*-
"""onnx_model_example_segmented (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PwnWGA0N52iXoAG3rY9MuYNglYnLgx8E
"""

# Copyright (c) Meta Platforms, Inc. and affiliates.

"""# Produces masks from prompts using an ONNX model

SAM's prompt encoder and mask decoder are very lightweight, which allows for efficient computation of a mask given user input. This notebook shows an example of how to export and use this lightweight component of the model in ONNX format, allowing it to run on a variety of platforms that support an ONNX runtime.
"""

from IPython.display import display, HTML
display(HTML(
"""
<a target="_blank" href="https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>
"""
))

"""## Environment Set-up

If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. The latest stable versions of PyTorch and ONNX are recommended for this notebook. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'.
"""

# Add time tracking and dataset creation imports
import time
import csv
import os
from datetime import datetime

# Create a dictionary to store performance metrics
performance_metrics = []

# Function to record metrics
def record_metric(operation, duration_ms, image_name=None, image_size=None, additional_info=None):
    metrics = {
        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'operation': operation,
        'duration_ms': duration_ms,
        'image_name': image_name,
        'image_size': image_size,
        'additional_info': additional_info
    }
    performance_metrics.append(metrics)
    print(f"✓ {operation} completed in {duration_ms:.2f} ms")

# Function to save metrics to CSV
def save_metrics_to_csv(filename='sam_performance_metrics.csv'):
    with open(filename, 'w', newline='') as csvfile:
        fieldnames = ['timestamp', 'operation', 'duration_ms', 'image_name', 'image_size', 'additional_info']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for metric in performance_metrics:
            writer.writerow(metric)
    print(f"✓ Performance metrics saved to {filename}")

using_colab = False

if using_colab:
    import torch
    import torchvision
    print("PyTorch version:", torch.__version__)
    print("Torchvision version:", torchvision.__version__)
    print("CUDA is available:", torch.cuda.is_available())
    import sys
    !{sys.executable} -m pip install opencv-python matplotlib onnx onnxruntime
    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'

    !mkdir images
    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg

    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

"""## Set-up

Note that this notebook requires both the `onnx` and `onnxruntime` optional dependencies, in addition to `opencv-python` and `matplotlib` for visualization.
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/ChaoningZhang/MobileSAM.git
# %cd MobileSAM
!pip install -e .

!pip install onnxruntime
!pip install onnx

import torch
import numpy as np
import cv2
import matplotlib.pyplot as plt
from mobile_sam import sam_model_registry, SamPredictor
from mobile_sam.utils.onnx import SamOnnxModel

import onnxruntime
from onnxruntime.quantization import QuantType
from onnxruntime.quantization.quantize import quantize_dynamic

def show_mask(mask, ax):
    color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)

def show_points(coords, labels, ax, marker_size=375):
    pos_points = coords[labels==1]
    neg_points = coords[labels==0]
    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)
    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)

def show_box(box, ax):
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))

"""## Export an ONNX model

Set the path below to a SAM model checkpoint, then load the model. This will be needed to both export the model and to calculate embeddings for the model.
"""

checkpoint = "/content/MobileSAM/weights/mobile_sam.pt"
model_type = "vit_t"

# Measure model loading time
start_time = time.time()
sam = sam_model_registry[model_type](checkpoint=checkpoint)
end_time = time.time()
model_loading_time = (end_time - start_time) * 1000  # Convert to milliseconds
record_metric("Model Loading", model_loading_time, additional_info=f"model_type={model_type}")

"""The script `segment-anything/scripts/export_onnx_model.py` can be used to export the necessary portion of SAM. Alternatively, run the following code to export an ONNX model. If you have already exported a model, set the path below and skip to the next section. Assure that the exported ONNX model aligns with the checkpoint and model type set above. This notebook expects the model was exported with the parameter `return_single_mask=True`."""

onnx_model_path = None  # Set to use an already exported model, then skip to the next section.

import warnings

onnx_model_path = "sam_onnx_example.onnx"

# Measure ONNX model creation time
start_time = time.time()
onnx_model = SamOnnxModel(sam, return_single_mask=True)

dynamic_axes = {
    "point_coords": {1: "num_points"},
    "point_labels": {1: "num_points"},
}

embed_dim = sam.prompt_encoder.embed_dim
embed_size = sam.prompt_encoder.image_embedding_size
mask_input_size = [4 * x for x in embed_size]
dummy_inputs = {
    "image_embeddings": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),
    "point_coords": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),
    "point_labels": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),
    "mask_input": torch.randn(1, 1, *mask_input_size, dtype=torch.float),
    "has_mask_input": torch.tensor([1], dtype=torch.float),
    "orig_im_size": torch.tensor([1500, 2250], dtype=torch.float),
}
output_names = ["masks", "iou_predictions", "low_res_masks"]

with warnings.catch_warnings():
    warnings.filterwarnings("ignore", category=torch.jit.TracerWarning)
    warnings.filterwarnings("ignore", category=UserWarning)
    with open(onnx_model_path, "wb") as f:
        torch.onnx.export(
            onnx_model,
            tuple(dummy_inputs.values()),
            f,
            export_params=True,
            verbose=False,
            opset_version=16,
            do_constant_folding=True,
            input_names=list(dummy_inputs.keys()),
            output_names=output_names,
            dynamic_axes=dynamic_axes,
        )
end_time = time.time()
onnx_export_time = (end_time - start_time) * 1000  # Convert to milliseconds
record_metric("ONNX Model Export", onnx_export_time, additional_info=f"model_type={model_type}")

"""If desired, the model can additionally be quantized and optimized. We find this improves web runtime significantly for negligible change in qualitative performance. Run the next cell to quantize the model, or skip to the next section otherwise."""

onnx_model_quantized_path = "sam_onnx_quantized_example.onnx"

# Measure quantization time
start_time = time.time()
quantize_dynamic(
    model_input=onnx_model_path,
    model_output=onnx_model_quantized_path,
    per_channel=False,
    reduce_range=False,
    weight_type=QuantType.QUInt8,
)
end_time = time.time()
quantization_time = (end_time - start_time) * 1000  # Convert to milliseconds
record_metric("ONNX Model Quantization", quantization_time)

onnx_model_path = onnx_model_quantized_path

"""## Example Image"""

# Measure image loading time
start_time = time.time()
image = cv2.imread('/content/images.jpeg')
if image is None:
    print("❌ Image not found or couldn't be read!")
else:
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
end_time = time.time()
image_loading_time = (end_time - start_time) * 1000  # Convert to milliseconds
image_size = f"{image.shape[1]}x{image.shape[0]}" if image is not None else "N/A"
record_metric("Image Loading", image_loading_time, image_name="images.jpeg", image_size=image_size)

plt.figure(figsize=(10,10))
plt.imshow(image)
plt.axis('on')
plt.show()

"""## Using an ONNX model

Here as an example, we use `onnxruntime` in python on CPU to execute the ONNX model. However, any platform that supports an ONNX runtime could be used in principle. Launch the runtime session below:
"""

# Measure ONNX runtime session initialization time
start_time = time.time()
ort_session = onnxruntime.InferenceSession(onnx_model_path)
end_time = time.time()
ort_init_time = (end_time - start_time) * 1000  # Convert to milliseconds
record_metric("ONNX Runtime Initialization", ort_init_time)

"""To use the ONNX model, the image must first be pre-processed using the SAM image encoder. This is a heavier weight process best performed on GPU. SamPredictor can be used as normal, then `.get_image_embedding()` will retreive the intermediate features."""

sam.to(device='cpu')
predictor = SamPredictor(sam)

# Measure image set time
start_time = time.time()
predictor.set_image(image)
end_time = time.time()
set_image_time = (end_time - start_time) * 1000  # Convert to milliseconds
record_metric("Set Image in Predictor", set_image_time, image_name="images.jpeg", image_size=image_size)

# Measure embedding generation time
start_time = time.time()
image_embedding = predictor.get_image_embedding().cpu().numpy()
end_time = time.time()
embedding_time = (end_time - start_time) * 1000  # Convert to milliseconds
record_metric("Image Embedding Generation", embedding_time, image_name="images.jpeg", image_size=image_size)

image_embedding.shape

"""The ONNX model has a different input signature than `SamPredictor.predict`. The following inputs must all be supplied. Note the special cases for both point and mask inputs. All inputs are `np.float32`.
* `image_embeddings`: The image embedding from `predictor.get_image_embedding()`. Has a batch index of length 1.
* `point_coords`: Coordinates of sparse input prompts, corresponding to both point inputs and box inputs. Boxes are encoded using two points, one for the top-left corner and one for the bottom-right corner. *Coordinates must already be transformed to long-side 1024.* Has a batch index of length 1.
* `point_labels`: Labels for the sparse input prompts. 0 is a negative input point, 1 is a positive input point, 2 is a top-left box corner, 3 is a bottom-right box corner, and -1 is a padding point. *If there is no box input, a single padding point with label -1 and coordinates (0.0, 0.0) should be concatenated.*
* `mask_input`: A mask input to the model with shape 1x1x256x256. This must be supplied even if there is no mask input. In this case, it can just be zeros.
* `has_mask_input`: An indicator for the mask input. 1 indicates a mask input, 0 indicates no mask input.
* `orig_im_size`: The size of the input image in (H,W) format, before any transformation.

Additionally, the ONNX model does not threshold the output mask logits. To obtain a binary mask, threshold at `sam.mask_threshold` (equal to 0.0).

### Example point input
"""

input_point = np.array([[250, 375]])
input_label = np.array([1])

"""Add a batch index, concatenate a padding point, and transform."""

onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]
onnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)

onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)

"""Create an empty mask input and an indicator for no mask."""

onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)
onnx_has_mask_input = np.zeros(1, dtype=np.float32)

"""Package the inputs to run in the onnx model"""

ort_inputs = {
    "image_embeddings": image_embedding,
    "point_coords": onnx_coord,
    "point_labels": onnx_label,
    "mask_input": onnx_mask_input,
    "has_mask_input": onnx_has_mask_input,
    "orig_im_size": np.array(image.shape[:2], dtype=np.float32)
}

"""Predict a mask and threshold it."""

# Measure point input prediction time
start_time = time.time()
masks, _, low_res_logits = ort_session.run(None, ort_inputs)
masks = masks > predictor.model.mask_threshold
end_time = time.time()
point_prediction_time = (end_time - start_time) * 1000  # Convert to milliseconds
record_metric("Point Input Prediction", point_prediction_time, image_name="images.jpeg", 
              additional_info=f"num_points={len(input_point)}")

masks.shape

plt.figure(figsize=(10,10))
plt.imshow(image)
show_mask(masks, plt.gca())
show_points(input_point, input_label, plt.gca())
plt.axis('off')
plt.show()

"""### Example mask input"""

input_point = np.array([[250, 375], [490, 380], [375, 360]])
input_label = np.array([1, 1, 0])

# Use the mask output from the previous run. It is already in the correct form for input to the ONNX model.
onnx_mask_input = low_res_logits

"""Transform the points as in the previous example."""

onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]
onnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)

onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)

"""The `has_mask_input` indicator is now 1."""

onnx_has_mask_input = np.ones(1, dtype=np.float32)

"""Package inputs, then predict and threshold the mask."""

ort_inputs = {
    "image_embeddings": image_embedding,
    "point_coords": onnx_coord,
    "point_labels": onnx_label,
    "mask_input": onnx_mask_input,
    "has_mask_input": onnx_has_mask_input,
    "orig_im_size": np.array(image.shape[:2], dtype=np.float32)
}

# Measure mask input prediction time
start_time = time.time()
masks, _, _ = ort_session.run(None, ort_inputs)
masks = masks > predictor.model.mask_threshold
end_time = time.time()
mask_prediction_time = (end_time - start_time) * 1000  # Convert to milliseconds
record_metric("Mask Input Prediction", mask_prediction_time, image_name="images.jpeg", 
              additional_info=f"num_points={len(input_point)}, has_mask=True")

plt.figure(figsize=(10,10))
plt.imshow(image)
show_mask(masks, plt.gca())
show_points(input_point, input_label, plt.gca())
plt.axis('off')
plt.show()

"""### Example box and point input"""

input_box = np.array([210, 200, 350, 500])
input_point = np.array([[275, 400]])
input_label = np.array([0])

"""Add a batch index, concatenate a box and point inputs, add the appropriate labels for the box corners, and transform. There is no padding point since the input includes a box input."""

onnx_box_coords = input_box.reshape(2, 2)
onnx_box_labels = np.array([2,3])

onnx_coord = np.concatenate([input_point, onnx_box_coords], axis=0)[None, :, :]
onnx_label = np.concatenate([input_label, onnx_box_labels], axis=0)[None, :].astype(np.float32)

onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)

"""Package inputs, then predict and threshold the mask."""

onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)
onnx_has_mask_input = np.zeros(1, dtype=np.float32)

ort_inputs = {
    "image_embeddings": image_embedding,
    "point_coords": onnx_coord,
    "point_labels": onnx_label,
    "mask_input": onnx_mask_input,
    "has_mask_input": onnx_has_mask_input,
    "orig_im_size": np.array(image.shape[:2], dtype=np.float32)
}

# Measure box and point input prediction time
start_time = time.time()
masks, _, _ = ort_session.run(None, ort_inputs)
masks = masks > predictor.model.mask_threshold
end_time = time.time()
box_prediction_time = (end_time - start_time) * 1000  # Convert to milliseconds
record_metric("Box and Point Input Prediction", box_prediction_time, image_name="images.jpeg", 
              additional_info=f"num_points={len(input_point)}, has_box=True")

plt.figure(figsize=(10, 10))
plt.imshow(image)
show_mask(masks[0], plt.gca())
show_box(input_box, plt.gca())
show_points(input_point, input_label, plt.gca())
# plt.axis('off')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/facebookresearch/segment-anything.git
# %cd segment-anything
!pip install -e .

import torch
from mobile_sam import sam_model_registry

# Path to the .pt model
checkpoint = "/content/MobileSAM/weights/mobile_sam.pt"

# Load the model from checkpoint
model_type = "vit_t"  # Use the appropriate model type (e.g., vit_t, vit_b, etc.)
model = sam_model_registry[model_type](checkpoint=checkpoint)
model.eval()

import torch
from mobile_sam import sam_model_registry

# Path to the .pt model
checkpoint = "/content/MobileSAM/weights/mobile_sam.pt"

# Load the model from checkpoint
model_type = "vit_t"
sam_model = sam_model_registry[model_type](checkpoint=checkpoint)
sam_model.eval()

# Create a wrapper class to simplify the interface for ONNX export
class MobileSAMWrapper(torch.nn.Module):
    def __init__(self, sam_model):
        super().__init__()
        self.sam_model = sam_model

    def forward(self, image, multimask_output=False):
        # Create the input format that the model expects
        batched_input = [{"image": image}]

        # Call the original model with the properly formatted input
        with torch.no_grad():
            # Skip the preprocess step since we'll handle normalization outside
            image_embeddings = self.sam_model.image_encoder(image)

            # Get the other outputs from the rest of the model
            sparse_embeddings, dense_embeddings = self.sam_model.prompt_encoder(
                points=None,
                boxes=None,
                masks=None,
            )

            masks, scores = self.sam_model.mask_decoder(
                image_embeddings=image_embeddings,
                image_pe=self.sam_model.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=multimask_output,
            )

            return masks, scores, image_embeddings

# Create an instance of the wrapper
model = MobileSAMWrapper(sam_model)

# Define dummy input with correct shape
dummy_input = torch.randn(1, 3, 1024, 1024)

# Normalize the input (using your sample values)
pixel_mean = torch.tensor([0.485, 0.456, 0.406])
pixel_std = torch.tensor([0.229, 0.224, 0.225])
dummy_input = (dummy_input - pixel_mean[None, :, None, None]) / pixel_std[None, :, None, None]

# Set multimask_output flag
dummy_multimask_output = torch.tensor(False)

# Measure full model ONNX export time
start_time = time.time()
# Export the model to ONNX format
onnx_model_path = "/content/mobile_sam.onnx"
torch.onnx.export(
    model,
    (dummy_input, dummy_multimask_output),
    onnx_model_path,
    opset_version=13,
    input_names=["image", "multimask_output"],
    output_names=["masks", "scores", "image_embeddings"],
    dynamic_axes={
        'image': {0: 'batch_size'},
        'masks': {0: 'batch_size'},
        'scores': {0: 'batch_size'},
        'image_embeddings': {0: 'batch_size'}
    }
)
end_time = time.time()
full_model_export_time = (end_time - start_time) * 1000  # Convert to milliseconds
record_metric("Full Model ONNX Export", full_model_export_time, additional_info=f"model_type={model_type}")

import cv2
import numpy as np
import matplotlib.pyplot as plt
from mobile_sam import sam_model_registry, SamPredictor

# Path to the .pt model
model_path = "/content/MobileSAM/weights/mobile_sam.pt"

# Load the model from checkpoint
model_type = "vit_t"  # Use the appropriate model type
model = sam_model_registry[model_type](checkpoint=model_path)
model.eval()

# Create a predictor
predictor = SamPredictor(model)

# Load the image
image_path = "/content/images.jpeg"
image = cv2.imread(image_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Display the original image
plt.figure(figsize=(8, 8))
plt.imshow(image)
plt.title("Original Image")
plt.axis("off")
plt.show()

# Set the image in the predictor
predictor.set_image(image)

# Define a point prompt (center of the image)
point_coords = np.array([[image.shape[1]//2, image.shape[0]//2]])
point_labels = np.array([1])  # 1 indicates a foreground point

# Measure predictor.predict time
start_time = time.time()
# Get masks from the model
masks, scores, logits = predictor.predict(
    point_coords=point_coords,
    point_labels=point_labels,
    multimask_output=True  # Return multiple masks
)
end_time = time.time()
predict_time = (end_time - start_time) * 1000  # Convert to milliseconds
record_metric("Native Predictor.predict", predict_time, image_name="images.jpeg", 
              additional_info=f"point_coords=center, multimask_output=True")

# Visualize the masks
for i, (mask, score) in enumerate(zip(masks, scores)):
    plt.figure(figsize=(8, 8))
    plt.imshow(image)
    plt.imshow(mask, alpha=0.6, cmap='jet')
    plt.title(f"Mask {i+1}, Score: {score:.3f}")
    plt.axis("off")
    plt.show()

import cv2
import numpy as np
import matplotlib.pyplot as plt
from mobile_sam import sam_model_registry, SamPredictor

# Path to the .pt model
model_path = "/content/MobileSAM/weights/mobile_sam.pt"

# Load the model from checkpoint
model_type = "vit_t"  # Use the appropriate model type
model = sam_model_registry[model_type](checkpoint=model_path)
model.eval()

# Create a predictor
predictor = SamPredictor(model)

# Load the image
image_path = "/content/images.jpeg"
image = cv2.imread(image_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Display the original image
plt.figure(figsize=(8, 8))
plt.imshow(image)
plt.title("Original Image")
plt.axis("off")
plt.show()

# Set the image in the predictor
predictor.set_image(image)

# Define a point prompt (center of the image)
point_coords = np.array([[image.shape[1]//2, image.shape[0]//2]])
point_labels = np.array([1])  # 1 indicates a foreground point

# Measure full segmentation time
start_time = time.time()
# Get masks from the model
masks, scores, logits = predictor.predict(
    point_coords=point_coords,
    point_labels=point_labels,
    multimask_output=True  # Return multiple masks
)

# Segment the image (extract the object)
segmented_images = []
for i, (mask, score) in enumerate(zip(masks, scores)):
    # Create a segmented version (object only)
    segmented_image = np.zeros_like(image)
    segmented_image[mask] = image[mask]
    segmented_images.append(segmented_image)

    # Create a version with the background removed (transparent background)
    rgba_image = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)
    rgba_image[:,:,0:3] = image
    rgba_image[:,:,3] = mask.astype(np.uint8) * 255

    # Save the segmented images
    cv2.imwrite(f"/content/segmented_object_{i+1}.png", cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR))

    # For RGBA (transparent background) version
    from PIL import Image
    Image.fromarray(rgba_image).save(f"/content/segmented_transparent_{i+1}.png")
end_time = time.time()
segmentation_time = (end_time - start_time) * 1000  # Convert to milliseconds
record_metric("Full Segmentation Process", segmentation_time, image_name="images.jpeg", 
              additional_info=f"multimask_output=True, num_masks={len(masks)}")

# Display the segmented image (object only)
for i, (segmented_image, score) in enumerate(zip(segmented_images, scores)):
    plt.figure(figsize=(8, 8))
    plt.imshow(segmented_image)
    plt.title(f"Segmented Object {i+1}, Score: {score:.3f}")
    plt.axis("off")
    plt.show()

# Save all performance metrics to CSV
save_metrics_to_csv()

# Print a summary of performance metrics
print("\n===== PERFORMANCE METRICS SUMMARY =====")
for metric in performance_metrics:
    print(f"{metric['operation']}: {metric['duration_ms']:.2f} ms")
print("=======================================")